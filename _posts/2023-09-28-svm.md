---
layout: post
title: ML SVM
date: 2023-09-28 20:08:02 +/-5000
categories: [ML]
tags: [svm]     # TAG names should always be lowercase
---

# SVM 

SVM was the best model before Neural Network. 

## Intuition 
Let's quickly recap how to represent a hyperplane in n-D space. 

In high school, we learned a line in 2D space can be represented as: 
$$y = ax+ b $$ 
This is equivalent to:
$$ w_1 x + w_2 y + b = 0 $$ 
For a plane in 3D: 
$$ w_1 x + w_2 y + w_3 z + b = 0 $$ 
For a hyperplane in n-D: 
$$ w_1 x_1 + w_2 x_2 + w_3 x_3 + ...+ w_n x_n + b = 0 $$ 
Which is equivalent to: 
$$ w^T x + b = 0$$ 
This is the general form of a line, plane, and hyperplane. Here, $w \in R^{n \times 1}$. If $n=2$, it's a line; if $n=3$, it's a plane; if $n > 3$, it's a hyperplane. 


Alternatively: 
$$ \theta^T x = 0, \theta \in R^{n+1 \times 1} $$ 

So, let's first try to investigate SVM in 2-D space. 

<!-- add an image here  -->

A line divides points into 2 classes. 
Thus, our decision function is:  
$$ h_{w,b} = g(w^T x + b) $$ 

$g(z) =1$, if $z \geq 0$; $g(z) = -1$, if $z < 0$ 

## Margin 
To know the confidence level of our classification result, the first intuition might be: directly input the point's coordinates into the line and use the magnitude and sign of the result to tell the correctness and confidence of our classification. 

Suppose the point is $(x_1, x_2)$, and the classification result is $y_1$. 

Here is the definition of **funciton margin**
$$ \hat\gamma = y (w^T x + b ) $$  

However, this margin is not ideal. If we scale both $w$ and $b$ up, the hyperplane remains unchanged, but the function margin scales up. 

So, here comes the definition of **geometric margin**, which is the distance from the data point to the hyperplane. 
$$ \gamma = \frac{w^T(x+b)}{\|w\|}$$ 

Note: when $\|w\| = 1$, function margin = geometric margin. 


## The optimal margin classifier 
We only care about the min margin when the hyperplane is fixed. 

We try to find the hyperplane which maximize the minimum margin. 

Expressing the above in mathematical terms: 
$$
\begin{aligned}
\max_{\gamma  , w, b} \quad &   \gamma  \\ 
\textrm{s.t.} \quad &  y^{(i)}(w^Tx^{(i)}+b)  \geq \gamma  , i=1,...,m
\end{aligned}
$$ 
The above equation indicates:  
- we aim to maximize the $\gamma$ 
- $\gamma$ is the minimum margin acheivable for all traning exampls 
- Setting $\|w\|=1$ to ensure $\hat \gamma = \gamma$

$$
\begin{aligned}
\max_{\gamma  , w, b} \quad &  \frac {\hat \gamma}{\|w\|}  \\ 
\textrm{s.t.} \quad &  y^{(i)}(w^Tx^{(i)}+b)  \geq \gamma  , i=1,...,m
\end{aligned}
$$ 

This is form is still not convex, so we constraint $\hat \gamma =1$, by scaling up $w$ and $b$. 

$$
\begin{aligned}
\max_{\gamma  , w, b} \quad &  \frac {1}{\|w\|} \\ 
\textrm{s.t.} \quad &  y^{(i)}(w^Tx^{(i)}+b)  \geq 1 , i=1,...,m
\end{aligned}
$$ 

Equivalently: 
$$ 
\begin{aligned}
\min_{\gamma  , w, b} \quad &  \frac {1}{2}\|w\|^2 \\ 
\textrm{s.t.} \quad &  y^{(i)}(w^Tx^{(i)}+b)  \geq 1 , i=1,...,m
\end{aligned}
$$ 

### Outliers? 
We add a penalty term for outliers: 
$$ 
\begin{aligned}
\min_{\gamma  , w, b} \quad &  \frac {1}{2}\|w\|^2 + C \sum _{i=1}^{n} \xi_i\\b   
\textrm{s.t.} \quad &  y^{(i)}(w^Tx^{(i)}+b)  \geq 1-\xi_i ,\  i=1,...,m \\
& \xi_i \geq 1 
\end{aligned}
$$ 
- $C$: is the regularization parameter we can adjust, determine how much we want to penalize the outliers. Bigger $C$ means we places more emphasis on minimizing the classification error, potentially at the expense of a smaller margin. 
- $\xi_i$: is the slack variable. if $\xi_i = 0$, the data point is correctly classifed outside the margin. if $\xi_i > 0$, the data point is either misclassified or within the margin. 

## Lagrange duality
### Quick recap of Lagrange multipliers 
Consider the following problem
$$
\begin{aligned}
\min_{x} \quad & f(x)\\
\textrm{s.t.} \quad & h_i(x)=0, \ i=1,...,l\\
\end{aligned}
$$ 
Thus, **Lagrangian** $\mathcal{L}$ is defined as: 

$$ \mathcal{L}(x_1, x_2, \ldots, x_n, \lambda) = f(x_1, x_2, \ldots, x_n) + \lambda \cdot g(x_1, x_2, \ldots, x_n) $$ 
then set the partial derivatives of $\mathcal{L}$ with respect to each $x_i$ and $\lambda$ to zero:
$$
\begin{align*}
\frac{\partial \mathcal{L}}{\partial x_1} &= 0 \\
\frac{\partial \mathcal{L}}{\partial x_2} &= 0 \\
&\vdots \\
\frac{\partial \mathcal{L}}{\partial x_n} &= 0 \\
\frac{\partial \mathcal{L}}{\partial \lambda} &= 0
\end{align*}
$$ 
$\lambda$ can be interpreted as the rate of change of the maximum (or minimum) value of the original function as the constraint is varied.

### Apply Lagrange multiplier to SVM dual problem 

Given the primal problem for soft-margin SVM:

$$ 
\begin{aligned}
\min_{\gamma  , w, b} \quad &  \frac {1}{2}\|w\|^2 + C \sum _{i=1}^{n} \xi_i\\   
\textrm{s.t.} \quad &  y^{(i)}(w^Tx^{(i)}+b)  \geq 1-\xi_i ,\  i=1,...,m \\
& \xi_i \geq 1 
\end{aligned}
$$ 

We can form the Lagrangian:
$$\mathcal{L}(w, b, \xi, \alpha, \beta) = \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n \xi_i - \sum_{i=1}^n \alpha_i [y_i(w \cdot x_i + b) - 1 + \xi_i] - \sum_{i=1}^n \beta_i \xi_i$$

Form the min and max condition again, 
$$ \min_{w,\xi} \max_{\alpha \geq 0, \beta \geq 0} \mathcal{L}(w, b, \xi, \alpha, \beta) $$ 

Uder Slater condition, changing min and max order does not affect the optimal condition, 

$$ \max_{\alpha \geq 0, \beta \geq 0} de\min_{w,\xi} \mathcal{L}(w, b, \xi, \alpha, \beta) $$ 

Setting the derivatives of the Lagrangian to zero:

With respect to $w$:
$$\frac{\partial \mathcal{L}}{\partial w} = w - \sum_{i=1}^n \alpha_i y_i x_i = 0$$
This gives:
$$w = \sum_{i=1}^n \alpha_i y_i x_i$$

With respect to $b$:
$$\frac{\partial \mathcal{L}}{\partial b} = -\sum_{i=1}^n \alpha_i y_i = 0$$

With respect to $\xi_i$:
$$\frac{\partial \mathcal{L}}{\partial \xi_i} = C - \alpha_i - \beta_i = 0$$

## Kernel Tricks 