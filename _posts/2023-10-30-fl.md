---
layout: post
title: Federated learning project 
date: 2023-10-30 20:08:02 +/-5000
categories: [ML]
tags: [federated_learning, project]     # TAG names should always be lowercase
---

## Plan 

|Time|Plan|
|-|-| 
|Week 1| [√] Literature Review, 3 papers for drift <br> [√] Finish the basic version of FL (without caring about the drift) | 
|Week 2| **Math** reason of "drift", <br> Write literature review part of the final report <br>  Conduct pre-experiments | 
|Week 3| Brain storm the improvements for solving drift| 
|Week 4| Implement new improvments| 
|Week 5| Finish report <br> Rehearse presentation| 

## Literature review 
All papers discussed below are based on **FedAvg** learning scheme. 
![Desktop View](/assets/atch/FL/fedavg.png)

The problem focused at is **drift** during aggregation. 
![Desktop View](/assets/atch/FL/drift.png)
### SCAFFOLD

![Desktop View](/assets/atch/FL/scaffold_correction.png)

### DFL: Decentralized FL

![Desktop View](/assets/atch/FL/dfl.png)

Based on my reading, there are mainly 3 ways to handle drift.
- Post-drift correction
    
    The correction can happen at any time between "data sampling, local learning, aggregation" 
  - Learnable batch size 对数据采样，减少noniid程度
  - 全局和局部目标的对齐：paper 1 scaffold，修改client目标以与期望的全局目标对齐，从而减轻模型漂移。
  - 加权聚合：依赖client data为局部模型分配权重来实现聚合
- 知识蒸馏：paper2 / 元学习/知识迁移
- x->z->y: only learn the latent space 

Also, in DFL paper, some *KPIs* are proposed to measure the FL, see below: 
![Desktop View](/assets/atch/FL/kpi.png)

### Reference 
Karimireddy, S. P., Kale, S., Mohri, M., Reddi, S. J., Stich, S. U., & Suresh, A. T. (2021). SCAFFOLD: Stochastic Controlled Averaging for Federated Learning. arXiv preprint arXiv:1910.06378.

Beltrán, E. T. M., Pérez, M. Q., Sánchez, P. M. S., Bernal, S. L., Bovet, G., Pérez, M. G., Pérez, G. M., & Celdrán, A. H. (2023). Decentralized Federated Learning: Fundamentals, State of the Art, Frameworks, Trends, and Challenges. IEEE Communications Surveys & Tutorials. Institute of Electrical and Electronics Engineers (IEEE). https://doi.org/10.1109/comst.2023.3315746